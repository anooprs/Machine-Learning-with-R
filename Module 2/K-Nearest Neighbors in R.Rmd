---
title: "KNN in R"
author: "Anoop"
date: "14/07/2021"
output: html_document
---

```
we are going to introduce the  𝐾 -nearest neighbors (KNN) algorithm and show some practical ways of using it in R with the knn function that exists in the class library.we are going to introduce the  𝐾 -nearest neighbors (KNN) algorithm and show some practical ways of using it in R with the knn function that exists in the class library.

```
***Import libraries***

```{r, warning=FALSE}

library(class)
library(caret)
require(mlbench)
library(e1071)
library(base)
```

***Step 1: Data collection***

For this lesson, we will be using Sonar data set (signals) from mlbench library. Sonar is a system for the detection of objects under water and for measuring the water's depth by emitting sound pulses and detecting. The complete description can be found in mlbench. For our purposes, this is a two-class (class  𝑅  and class  𝑀 ) classification task with numeric data.

Let's look at the first five rows of Sonar-

```{r}
data(Sonar)
head(Sonar)
```

***Step 2: Prepare and explore data***

It is A data frame with 208 observations on 61 variables, all numerical and one (the Class) nominal.

```{r}
cat("number of rows and columns are:", nrow(Sonar), ncol(Sonar))
```
Lets check how many  𝑀  classes and  𝑅  classes Sonar data contain? and check whether Sonar data contains any NA in its columns.

```{r}
table(Sonar$Class) 
apply(Sonar, 2, function(x) sum(is.na(x))) 
```
Here, we want to manually take samples from our data to split Sonar into training and test sets.

```{r}
SEED <- 123
set.seed(SEED)
data <- Sonar[sample(nrow(Sonar)), ] # shuffle data first
bound <- floor(0.7 * nrow(data))
df_train <- data[1:bound, ] 
df_test <- data[(bound + 1):nrow(data), ]
cat("number of training and test samples are ", nrow(df_train), nrow(df_test))
```
Let's examine if the train and test samples have properly splitted with the almost the same portion of Class labels

```{r}
cat("number of training classes: \n", table(df_train$Class)/nrow(df_train))
cat("\n")
cat("number of test classes: \n", table(df_test$Class)/nrow(df_test))
```
Let's create dataframes of train and test to simplify our task:

```{r}
X_train <- subset(df_train, select=-Class)
y_train <- df_train$Class
X_test <- subset(df_test, select=-Class) # exclude Class for prediction
y_test <- df_test$Class
```

***Step 3:Training a model on data***

```{r}
model_knn <- knn(train=X_train,
                 test=X_test,
                 cl=y_train,  # class labels
                 k=3)
model_knn
```

***Step 4: Evaluate the model performance***

As you can see, model_knn with  𝑘=3  provides the above predictions for the test set X_test. Then, we can see how many classes have been correctly or incorrectly classified by comparing to the true labels as follows-

```{r}
conf_mat <- table(y_test, model_knn)
conf_mat
```
To compute the accuracy, we sum up all the correctly classified observations (located in diagonal) and divide it by the total number of classes

```{r}
cat("Test accuracy: ", sum(diag(conf_mat))/sum(conf_mat))
```

To assess whether  𝑘=3  is a good choice and see whether  𝑘=3  leads to overfitting /underfitting the data, we could use knn.cv which does the leave-one-out cross-validations for training set (i.e., it singles out a training sample one at a time and tries to view it as a new example and see what class label it assigns).

Below are the predicted classes for the training set using the leave-one-out cross-validation. Now, let's examine its accuracy

```{r}
knn_loocv <- knn.cv(train=X_train, cl=y_train, k=3)
knn_loocv
```

Lets create a confusion matrix to compute the accuracy of the training labels y_train and the cross-validated predictions knn_loocv, same as the above. What can you find from comparing the LOOCV accuracy and the test accuracy above?

```{r}
conf_mat_cv <- table(y_train, knn_loocv)
conf_mat_cv
cat("LOOCV accuracy: ", sum(diag(conf_mat_cv)) / sum(conf_mat_cv))
```
The difference between the cross-validated accuracy and the test accuracy shows that,  𝑘=3  leads to overfitting. Perhaps we should change  𝑘  to lessen the overfitting.

***Step 5: Improve the performance of the model***

As noted earlier, we have not standardized (as part of preprocessing) our training and test sets. In the rest of the tutorial, we will see the effect of choosing a suitable  𝑘  through repeated cross-validations using caret library.

In a cross-validation procedure:

The data is divided into the finite number of mutually exclusive subsets
Through each iteration, a subset is set aside, and the remaining subsets are used as the training set
The subset that was set aside is used as the test set (prediction)
This is a method of cross-referencing the model built using its own data.

```{r}
SEED <- 2016
set.seed(SEED)

# create the training data 70% of the overall Sonar data.

in_train <- createDataPartition(Sonar$Class, p=0.7, list=FALSE) # create training indices

ndf_train <- Sonar[in_train, ]
ndf_test <- Sonar[-in_train, ]
```
Here, we specify the cross-validation method we want to use to find the best  𝑘  in grid search. Later, we use the built-in plot function to assess the changes in accuracy for different choices of  𝑘.

```{r}
# lets create a function setup to do 5-fold cross-validation with 2 repeat.
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)

nn_grid <- expand.grid(k=c(1,3,5,7))
nn_grid
```
```{r}
set.seed(SEED)

best_knn <- train(Class~., data=ndf_train,
                  method="knn",
                  trControl=ctrl, 
                  preProcess = c("center", "scale"),  # standardize
                  tuneGrid=nn_grid)
best_knn
```
So seemingly,  𝑘=1  has the highest accuracy from repeated cross-validation.

Let's try to do dimensionality reduction as part of preprocess to achieve higher testing accuracy than above. This may not have a definite solution and it depends on how hard you try!

Use the above best_knn to make predictions on the test set (remember to remove the Class for prediction). Then create the much better version of confusion matrix with confusionMatrix function from caret and examine the accuracy and its  %95  confidence interval.

In fact, the above result indicates  𝑘=1  (as could be guessed) is also overfitting, though it might be a better option than  𝑘=3.  Since the initial dimension of our data is high ( 61  is considered high!), then you might have suspected the better approach, is to preform dimensionality reduction as part of preprocessing.

```{r}
SEED <- 123 
set.seed(SEED) 

ctrl <- trainControl(method="repeatedcv", number=5, repeats=5) 
nn_grid <- expand.grid(k=c(1, 3, 5, 7)) 
best_knn_reduced <- train( Class~., data=ndf_train, method="knn", 
                            trControl=ctrl, preProcess=c("center", "scale","YeoJohnson"))

X_test <- subset(ndf_test, select=-Class) 

pred_reduced <- predict(best_knn_reduced, newdata=X_test, model="best") 
conf_mat_best_reduced <- confusionMatrix(ndf_test$Class, pred_reduced) 

conf_mat_best_reduced 

```

***END***